{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZdhqLBI3oYi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from env import CustomEnvironment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K67Sa5n3yAU"
      },
      "outputs": [],
      "source": [
        "actions_size = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my61EZHK30gs"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(environment) :\n",
        "\n",
        "    input = []\n",
        "\n",
        "    for i in range(len(environment.state)):\n",
        "      input.append(environment.state[i] / environment.grid_size[i])\n",
        "\n",
        "    for i in range(len(environment.destination)):\n",
        "      input.append(environment.destination[i] / environment.grid_size[i])\n",
        "\n",
        "    for i in range(len(environment.actions)):\n",
        "      input.append(0)\n",
        "\n",
        "    input[len(environment.state) + environment.direction + 1] = 1\n",
        "\n",
        "    input.append(environment.velocity / environment.max_velocity)\n",
        "\n",
        "    input.append(environment.communication_radius / math.sqrt(environment.grid_size[0] ** 2 + environment.grid_size[1] ** 2))\n",
        "\n",
        "    input = np.array(input)\n",
        "\n",
        "    return input # x, y, xd, yd, direction, velocity, com_rad\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0Clfswm32E-"
      },
      "outputs": [],
      "source": [
        "def get_nearby_vehicles(environment):\n",
        "    nearby_vehicles = []\n",
        "\n",
        "    for i in environment.stationary_vehicles:\n",
        "      distance = math.sqrt((environment.state[0] - i.state[0])** 2 + (environment.state[1] - i.state[1])** 2)\n",
        "      if(distance <= environment.communication_radius or distance <= i.communication_radius):\n",
        "        nearby_vehicles.append(i)\n",
        "\n",
        "    return nearby_vehicles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RImFsa37335a"
      },
      "outputs": [],
      "source": [
        "def get_nearby_vehicle_input(environment) :\n",
        "    nearby_vehicles = get_nearby_vehicles(environment)\n",
        "    nearby_vehicles_input = []\n",
        "    for i in nearby_vehicles :\n",
        "      nearby_vehicles_input.append(preprocess_data(i))\n",
        "\n",
        "    nearby_vehicles_input = np.array(nearby_vehicles_input)\n",
        "\n",
        "    return nearby_vehicles_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtACHuUn36B3"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import LSTM, Dense, Input, Concatenate\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7vRwZA9370P"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "\n",
        "    input_size = 10\n",
        "    lstm_hidden_size = 100\n",
        "\n",
        "    current_vehicle_input = Input(shape=(input_size, ), name='current_vehicle_input')\n",
        "    nearby_vehicles_input = Input(shape=(None, input_size), name='nearby_vehicles_input')\n",
        "\n",
        "    lstm_layer = LSTM(lstm_hidden_size, return_sequences=False)(nearby_vehicles_input)\n",
        "\n",
        "    current_vehicle_layer = Dense(64, activation='relu')(current_vehicle_input)\n",
        "\n",
        "    combined_output = Concatenate()([lstm_layer, current_vehicle_layer])\n",
        "\n",
        "    output_layer = Dense(actions_size, activation='softmax')(combined_output)\n",
        "\n",
        "    model = Model(inputs=[current_vehicle_input, nearby_vehicles_input], outputs=output_layer)\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHDjeBRh39jX",
        "outputId": "73affb82-9fcb-41e7-8af2-5f9b3c258faf"
      },
      "outputs": [],
      "source": [
        "model = build_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfdsMokF3_jt"
      },
      "outputs": [],
      "source": [
        "num_episodes = 1000\n",
        "max_steps_per_episode = 100\n",
        "\n",
        "discount_rate = 0.99\n",
        "learning_rate = 0.1\n",
        "\n",
        "exploration_rate = 1\n",
        "exploration_decay_rate = 0.01\n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T320jhmb4Bzh"
      },
      "outputs": [],
      "source": [
        "rewards_all_episodes = []\n",
        "input_size = 10\n",
        "env = CustomEnvironment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmUlsY9E4Dx7",
        "outputId": "624b9f51-3886-4fb0-9fd5-fbd3d386257e"
      },
      "outputs": [],
      "source": [
        "\n",
        "for episode in range(num_episodes) :\n",
        "\n",
        "    state = env.reset()\n",
        "    rewards_current_episode = 0\n",
        "\n",
        "    for step in range(max_steps_per_episode):\n",
        "\n",
        "\n",
        "            current_vehicle_input = preprocess_data(state)\n",
        "            nearby_vehicles_input = get_nearby_vehicle_input(state)\n",
        "\n",
        "            exploration_rate_threshold = random.uniform(0, 1)\n",
        "\n",
        "            if exploration_rate_threshold < exploration_rate:\n",
        "                action = random.choice(state.actions)\n",
        "                q_values = [0] * len(state.actions)\n",
        "                q_values[action] = 1\n",
        "                q_values = np.array(q_values, dtype=np.float64)\n",
        "                q_values = [q_values]\n",
        "                q_values = np.array(q_values)\n",
        "            else:\n",
        "                q_values = model.predict([current_vehicle_input.reshape(1, -1), nearby_vehicles_input.reshape(1, -1, input_size)], verbose=0)\n",
        "                action = np.argmax(q_values)\n",
        "\n",
        "            new_state, reward, done = state.step(action)\n",
        "\n",
        "            rewards_current_episode += reward\n",
        "\n",
        "            new_current_vehicle_input = preprocess_data(new_state)\n",
        "            new_nearby_vehicles_input = get_nearby_vehicle_input(new_state)\n",
        "            print(new_current_vehicle_input.shape, new_nearby_vehicles_input.shape)\n",
        "\n",
        "            if new_nearby_vehicles_input.shape[0] > 0 :\n",
        "              new_q_values = model.predict([new_current_vehicle_input.reshape(1, -1), new_nearby_vehicles_input.reshape(1, -1, input_size)], verbose=0)\n",
        "\n",
        "              target_q_value = reward + discount_rate * np.max(new_q_values)\n",
        "              q_values[0][action] = target_q_value\n",
        "\n",
        "              model.fit([current_vehicle_input.reshape(1, -1), nearby_vehicles_input.reshape(1, -1, input_size)], q_values, verbose=0)\n",
        "\n",
        "              if done :\n",
        "                break\n",
        "\n",
        "    print(episode, rewards_current_episode)\n",
        "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
        "\n",
        "    rewards_all_episodes.append(rewards_current_episode)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6otMySFMR-Y",
        "outputId": "15e9f422-839f-4989-adeb-399a45f12dce"
      },
      "outputs": [],
      "source": [
        "rewards_all_episodes"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
